{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IU2Oe2lgy3XV"
   },
   "source": [
    "# Web Scraping\n",
    "\n",
    "Lots of great data is available online, but available through inconveniently formatted web pages. When this is the case, sometimes you can simply copy and paste the data into a .csv file and go on with your life. But if there are many records to parse and combine into a dataset, that might be impossible. Can we automate the collection of data from online sources?\n",
    "\n",
    "This is called web scraping. Broadly speaking: Web scraping is legal, but what you plan to do with the results of your scraping might not be. In general, most sites do not want you to scrape them at this point, but there is not really a way to stop you if you are sufficiently motivated. Be careful to use server resources respectfully (not too many requests per unit time), think seriously about privacy concerns, and be careful who you share your work with and how it is used.\n",
    "\n",
    "We'll be scraping data about used cars in Charlottesville from Craigslist, particularly this page: https://charlottesville.craigslist.org/search/cta?purveyor=owner#search=1~gallery~0~0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ui3HNFVXy3XX"
   },
   "source": [
    "We'll use the `requests` package to get web pages off the Internet and into Python. Again, we'll use a header with a user-agent that masks our true identity so that we're not rejected by the server. This particular url points to the car listings for Craigslist in Charlottesville. To use requests, you pass a `url` for the page you want and a `header` that controls how you appear to the server to `requests.get`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bT63yifry3XX"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import requests # Page requests\n",
    "\n",
    "header = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:124.0) Gecko/20100101 Firefox/124.0'} # How we wish to appear to CL\n",
    "url = 'https://charlottesville.craigslist.org/search/cta?purveyor=owner#search=1~gallery~0~0' # The page we want to scrape\n",
    "raw = requests.get(url,headers=header) # Get page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GS1XkRlXy3XX"
   },
   "source": [
    "Now that we have that particular page available locally, we want to **parse** it and get results from it. To do that, we can use a package called `beautifulSoup` or `bs4`.\n",
    "\n",
    "What does `beautifulSoup` do for us? Let's go to the web page of interest. You probably see something like this:\n",
    "\n",
    "![Listings](./src/craigslist.png \"Craigslist\")\n",
    "\n",
    "But if you \"view page source\" -- which is CTRL+U -- in Chrome, you see what the computer sees:\n",
    "\n",
    "![Listings](./src/craigslist_source.png \"Craigslist\")\n",
    "\n",
    "Since your web browser needs lots of instructions about how to render the text, pictures, and other content on your web page, there are a lot of clues about where the data live and how to extricate them from a page. These clues are called **tags**. If you wander the source for the search page on cars, you see a particular `class = \"cl-static-search-result\"` term appear attached to each listing:\n",
    "\n",
    "![Listings](./src/listing.png \"Craigslist\")\n",
    "\n",
    "This structure can be exploited to search the page for information. This kind of detective work -- looking at the page source, finding the interesting tags, and then searching the page with `beautifulSoup` -- is the basic job of web scraping.\n",
    "\n",
    "What I do is look at the rendered page, find the text I want and copy it, then search for that text in the HTML code that the computer sees. When I find the text I want, I look for the tag nearby. Here, it's `cl-static-search-result`: That's what I'll want beautifulSoup to search for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fEO2NpZ8y3XY"
   },
   "source": [
    "The following code chunk takes the raw content from `requests` and turns it into a beautifulSoup object, which can search the page and return results for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lllobrYIy3XY"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as soup # HTML parser\n",
    "bsObj = soup(raw.content,'html.parser') # Parse the html\n",
    "listings = bsObj.find_all(class_=\"cl-static-search-result\") # Find all listings of the kind we want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ymO8JbMmy3XY"
   },
   "source": [
    "Why is the argument `class_` and not just `class`? The word `class` is a reserved keyword for Python, and cannot be used by anyone else, similar to `True` and `False`. But since we want the `class = \"cl-static-search-result\"` terms, we need to use the `class_` argument to the `.find_all` method.\n",
    "\n",
    "The `.find_all` function dredges the entire page and finds all the instances of `class = \"cl-static-search-result\"`, resulting in a list of entries. We can then parse the entries.\n",
    "\n",
    "Parsing the entries can be a challenge! We have to go back to what a listing looks like, and look at the tags within the listing. They're typically `div` tags with a `class` like price or location. You then have to experiment a bit with `.find` and the HTML to make sure you're getting the information you want. For each listing, the `.find` method to search within the listing record for specific information, but it's typically still wrapped in the tag. To get the real information we want, we can then use `.get_text()`. I end up using a code chunk to experiment and play with a record to make sure I'm getting as close to what I want as possible.\n",
    "\n",
    "In the code below, two more things happen. You don't need to do them in your work, but they're helpful to know about.\n",
    "\n",
    "First, I would like to get the brand of the car from the post title, if possible. To do this, I split the title into words using `title.split()`, and then I use a list comprehension to look over every word in the title and check whether it appears in the `brands` list.\n",
    "\n",
    "Second, I would like to get the year the car was built, if possible, so I can determine the vehicle's age. To do this, I use a thing called **regular expressions** that provides a language for expressing patterns. Do I remember how to do this off the top of my head? No, I read a few pages in a book and looked on StackOverflow for answers. Roughly, in order to express the idea \"any year starting with 20xx,\" you can write `20[0-9][0-9]`, and for \"any year starting with 19xx,\" you can write `19[0-9][0-9]`. The `[0-9]`'s act as wildcards for any digit. This allows me to use the `re` package to find any instances of year-like numbers in the title text, using `re.search(r'20[0-9][0-9]|19[0-9][0-9]', title )`.\n",
    "\n",
    "This is all nested in a for-loop over the listings, and the data is appended to a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jw-U4BA-y3XY"
   },
   "outputs": [],
   "source": [
    "import re # Regular expressions\n",
    "\n",
    "brands = ['honda', 'dodge','toyota','ford','tesla','gmc','jeep','bmw','mitsubishi','mazda',\n",
    "          'volvo','audi','volkswagen','chevy','chevrolet','acura','kia','subaru','lexus',\n",
    "          'cadillac','buick','porsche','infiniti']\n",
    "\n",
    "data = [] # We'll save our listings in this object\n",
    "for k in range( len(listings) ):\n",
    "    title = listings[k].find('div',class_='title').get_text().lower()\n",
    "    price = listings[k].find('div',class_='price').get_text()\n",
    "    link = listings[k].find(href=True)['href']\n",
    "    # Get brand from the title string:\n",
    "    words = title.split()\n",
    "    hits = [word for word in words if word in brands] # Find brands in the title\n",
    "    if len(hits) == 0:\n",
    "        brand = 'missing'\n",
    "    else:\n",
    "        brand = hits[0]\n",
    "    # Get years from title string:\n",
    "    regex_search = re.search(r'20[0-9][0-9]|19[0-9][0-9]', title ) # Find year references\n",
    "    if regex_search is None: # If no hits, record year as missing value\n",
    "        year = np.nan\n",
    "    else: # If hits, record year as first match\n",
    "        year = regex_search.group(0)\n",
    "    #\n",
    "    data.append({'title':title,'price':price,'year':year,'link':link,'brand':brand})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mz4zVi80y3XZ"
   },
   "source": [
    "\n",
    "With the data scraped from Craigslist, we can put it in a dataframe and wrangle it. Of course, price and year come in as text, not numbers, and need to be typecast/coerced:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KY1njgHcy3XZ",
    "outputId": "985126b7-e5e5-40c3-ba09-ed0b56625f69"
   },
   "outputs": [],
   "source": [
    "## Wrangle the data\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "df['price'] = df['price'].str.replace('$','')\n",
    "df['price'] = df['price'].str.replace(',','')\n",
    "df['price'] = pd.to_numeric(df['price'],errors='coerce')\n",
    "df['year'] = pd.to_numeric(df['year'],errors='coerce')\n",
    "df['age'] = 2025-df['year']\n",
    "print(df.shape)\n",
    "df.to_csv('./src/craigslist_cville_cars.csv') # Save data in case of a disaster\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YBcg4XUoy3XZ"
   },
   "source": [
    "With the data in and wrangled, we can now do some analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qBMwX_rFy3XZ",
    "outputId": "9abfa7a7-f4fb-4563-c0f6-531f74310c3e"
   },
   "outputs": [],
   "source": [
    "print(df['price'].describe())\n",
    "df['price'].hist(grid=False)\n",
    "plt.show()\n",
    "print(df['age'].describe())\n",
    "df['age'].hist(grid=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xsJPUF2wy3XZ",
    "outputId": "9a613e49-3b2b-4214-9ae3-9a5f0986ff6e"
   },
   "outputs": [],
   "source": [
    "# Price by brand:\n",
    "df.loc[:,['price','brand']].groupby('brand').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XP4Q0dIdy3Xa",
    "outputId": "883e5889-5d9f-472b-d686-12a257490c23"
   },
   "outputs": [],
   "source": [
    "# Age by brand:\n",
    "df.loc[:,['age','brand']].groupby('brand').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lHypfI-py3Xa",
    "outputId": "1f24c08e-e5d6-4f60-a944-e1760c3fd51d"
   },
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(data=df, x='age', y='price',hue='brand')\n",
    "sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LzFhDn50y3Xa",
    "outputId": "bd26a020-f944-4b07-f98f-40a64db9d94d"
   },
   "outputs": [],
   "source": [
    "df['log_price'] = np.log(df['price'])\n",
    "df['log_age'] = np.log(df['age'])\n",
    "\n",
    "ax = sns.scatterplot(data=df, x='log_age', y='log_price',hue='brand')\n",
    "sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "\n",
    "print(df.loc[:,['log_price','log_age']].cov())\n",
    "print(df.loc[:,['log_price','log_age']].corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Lab:\n",
    "\n",
    "\n",
    "1. Pick something else on Craigslist besides used cars: Musical instruments, roommates, antiques, etc. Look at the search page and its source code. Record which fields/data you would like to gather, and what kinds of basic EDA you'd do with it.\n",
    "2. Get your search results of interest using `requests` and extract data from them using `beautifulSoup`, using code similar to what's above.\n",
    "3. Wrangle your data into a dataframe and do some basic descriptions and plots. Try to find some interesting relationships or stories to tell about your data.\n",
    "4. Document all your work as code and markdown blocks in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
